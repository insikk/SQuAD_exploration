{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Is there an answer sentence?\n",
    "\n",
    "Finding a sentence containing answer span is necessary condition of well-performing model. If every answer span is contained in a single sentence, we could use this measure to test various model's performance. \n",
    "\n",
    "\n",
    "\n",
    "## Result\n",
    "\n",
    "### Training set\n",
    "\n",
    "number of problem, question pair: 87599\n",
    "\n",
    "number of questions answer span is not in a sentence: 216\n",
    "\n",
    "216 / 87599 = 0.2%\n",
    "\n",
    "\n",
    "### Dev set\n",
    "\n",
    "number of problem, question pair: 10570\n",
    "\n",
    "number of questions answer span is not in a sentence: 7\n",
    "\n",
    "7 / 10570 < 0.1 %\n",
    "\n",
    "\n",
    "\n",
    "## Requirement\n",
    "\n",
    "sentence segmentation: Extract sentences from a given paragraph. \n",
    "\n",
    "## Discussion\n",
    "\n",
    "[v] Q. Is it possible for perfect sentence segmentation? => No\n",
    "\n",
    "> When using nltk.tokenize.sent_tokenize(context)\n",
    "> It cannot correctly segmenting sentences below:\n",
    ">\n",
    "> The university founder, Fr.\n",
    "> Sorin and the president at the time, the Rev.\n",
    "> William Corby, immediately planned for the rebuilding of the structure that had housed virtually the entire University.\n",
    ">\n",
    ">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "# download nltk package for sentence segmentation\n",
    "\n",
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "work_dir = \"..\\\\data\\\\\"\n",
    "# train_filename = \"train-v1.1.json\"\n",
    "train_filename = \"dev-v1.1.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train datafile info:\n",
      "\tversion: 1.1\n",
      "number of problem, question pair: 10570\n",
      "number of questions answer span is not in a sentence: 7\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "with open(os.path.join(work_dir, train_filename)) as data_file:    \n",
    "    train_data = json.load(data_file)\n",
    "\n",
    "\n",
    "print(\"train datafile info:\")\n",
    "print(\"\\tversion:\", train_data[\"version\"])\n",
    "\n",
    "data = train_data[\"data\"]\n",
    "\n",
    "\n",
    "debug_print = True\n",
    "    \n",
    "def check_answer_span(data):\n",
    "    from nltk import tokenize\n",
    "    # import spacy                     \n",
    "    # nlp = spacy.load('en')\n",
    "    \n",
    "    problem_count = 0\n",
    "    answer_not_in_sentence_count = 0\n",
    "    for idx, article in enumerate(data):\n",
    "        title = article[\"title\"]\n",
    "        # print(title)\n",
    "        paragraphs = article[\"paragraphs\"]\n",
    "        for paragraph in paragraphs:\n",
    "            context = paragraph[\"context\"]\n",
    "            qas = paragraph[\"qas\"]\n",
    "            # doc = nlp(context)      \n",
    "            sentences = tokenize.sent_tokenize(context)\n",
    "            # print(sentences)\n",
    "            \n",
    "            for qa in qas:\n",
    "                id = qa[\"id\"]\n",
    "                question = qa[\"question\"]\n",
    "                answers = qa[\"answers\"]\n",
    "\n",
    "                \n",
    "\n",
    "                problem_count = problem_count + 1\n",
    "                is_answer_in_sentence = False\n",
    "                for answer in answers:\n",
    "                    for s in sentences:\n",
    "                        if answer['text'] in s:\n",
    "                            is_answer_in_sentence = True\n",
    "                # print(\"is answer in sentence?\", is_answer_in_sentence)\n",
    "                if not is_answer_in_sentence:\n",
    "                    if False and debug_print == True:\n",
    "                        print(id)\n",
    "                        print(context)\n",
    "                        print(question)\n",
    "                        print(answers)\n",
    "                        for s in sentences:\n",
    "                            print(s)\n",
    "                    answer_not_in_sentence_count = answer_not_in_sentence_count + 1\n",
    "        # print(\"%d done\" % (idx))\n",
    "        \n",
    "    print(\"number of problem, question pair:\", problem_count)\n",
    "    print(\"number of questions answer span is not in a sentence:\", answer_not_in_sentence_count)\n",
    "    return True\n",
    "        \n",
    "result = check_answer_span(data)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
