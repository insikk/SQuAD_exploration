{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Sentences\n",
    "\n",
    "Sometimes we want to train a model only with sentences to make sentence representation. To do that, we convert the training and dev dataset's sentences from context and questions in a separate file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev datafile info:\n",
      "\tversion: 1.1\n",
      "\n",
      "total number of sentences extracted from context: 10458\n",
      "total number of sentences extracted from question: 10570\n",
      "train datafile info:\n",
      "\tversion: 1.1\n",
      "\n",
      "total number of sentences extracted from context: 93595\n",
      "total number of sentences extracted from question: 87599\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "def get_sentence_spans(context):\n",
    "    from nltk.data import load\n",
    "    tokenizer = load('tokenizers/punkt/{0}.pickle'.format('english'))\n",
    "    sentence_span = tokenizer.span_tokenize(context)\n",
    "    return sentence_span\n",
    "\n",
    "def get_sentence_index(answer_span, sentence_spans):\n",
    "    \"\"\"\n",
    "    return the sentence index. It returns -1 if the answer span could not be found on a single sentence. \n",
    "    \"\"\"\n",
    "    idx = -1\n",
    "    for sent_idx, sent_span in enumerate(sentence_spans):\n",
    "        if sent_span[0] <= answer_span[0] and answer_span[1] <=sent_span[1]+1:\n",
    "            idx = sent_idx\n",
    "            break\n",
    "    return idx\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "work_dir = os.path.join(\"..\", \"data\")\n",
    "dev_filenames = {'dev': \"dev-v1.1.json\", 'train': \"train-v1.1.json\"}\n",
    "\n",
    "for key, filename in dev_filenames.items():\n",
    "    with open(os.path.join(work_dir, filename)) as data_file:    \n",
    "        filedata = json.load(data_file)\n",
    "\n",
    "\n",
    "    print(\"%s datafile info:\" % (key))\n",
    "    print(\"\\tversion:\", filedata[\"version\"])\n",
    "    print()\n",
    "\n",
    "    data = filedata[\"data\"]\n",
    "\n",
    "    sentences_context = []\n",
    "    sentences_question = []\n",
    "    for article in data:\n",
    "        title = article[\"title\"]\n",
    "        # print(title)\n",
    "        paragraphs = article[\"paragraphs\"]\n",
    "        for paragraph in paragraphs:\n",
    "            context = paragraph[\"context\"]\n",
    "            spans = get_sentence_spans(context)\n",
    "\n",
    "            # print context sentence by sentence\n",
    "            for idx, sent_span in enumerate(spans):\n",
    "                sentences_context.append(context[sent_span[0]:sent_span[1]+1])\n",
    "\n",
    "\n",
    "            qas = paragraph[\"qas\"]\n",
    "            for qa in qas:\n",
    "                id = qa[\"id\"]\n",
    "                question = qa[\"question\"]\n",
    "                answers = qa[\"answers\"]\n",
    "\n",
    "                sentences_question.append(question)\n",
    "\n",
    "    print(\"total number of sentences extracted from context:\", len(sentences_context))\n",
    "    print(\"total number of sentences extracted from question:\", len(sentences_question))\n",
    "    \n",
    "    with open(\"sentences_context_%s.txt\"%(key), \"w\",encoding='utf-8') as f:\n",
    "        for sent in sentences_context:\n",
    "            f.write(\"%s\\t%s\\tneutral\\n\"%(sent, sent))\n",
    "        \n",
    "    with open(\"sentences_question_%s.txt\"%(key), \"w\",encoding='utf-8') as f:\n",
    "        for sent in sentences_question:\n",
    "            f.write(\"%s\\t%s\\tneutral\\n\"%(sent, sent))\n",
    "\n",
    "    with open(\"sentences_all_%s.txt\"%(key), \"w\",encoding='utf-8') as f:\n",
    "        for sent in sentences_context + sentences_question:\n",
    "            f.write(\"%s\\t%s\\tneutral\\n\"%(sent, sent))\n",
    "\n",
    "    sentences_question[:5]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert in into encoder decoder format\n",
    "\n",
    "Create a file that contains 3 tab separated entity.\n",
    "\n",
    "sentence1 sentence2 dummy_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
