{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Sentences\n",
    "\n",
    "Sometimes we want to train a model only with sentences to make sentence representation. To do that, we convert the training and dev dataset's sentences from context and questions in a separate file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train datafile info:\n",
      "\tversion: 1.1\n",
      "\n",
      "train datafile info:\n",
      "\tversion: 1.1\n",
      "\n",
      "total number of sentences extracted from context: 93595\n",
      "total number of sentences extracted from question: 87599\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',\n",
       " 'What is in front of the Notre Dame Main Building?',\n",
       " 'The Basilica of the Sacred heart at Notre Dame is beside to which structure?',\n",
       " 'What is the Grotto at Notre Dame?',\n",
       " 'What sits on top of the Main Building at Notre Dame?']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "def get_sentence_spans(context):\n",
    "    from nltk.data import load\n",
    "    tokenizer = load('tokenizers/punkt/{0}.pickle'.format('english'))\n",
    "    sentence_span = tokenizer.span_tokenize(context)\n",
    "    return sentence_span\n",
    "\n",
    "def get_sentence_index(answer_span, sentence_spans):\n",
    "    \"\"\"\n",
    "    return the sentence index. It returns -1 if the answer span could not be found on a single sentence. \n",
    "    \"\"\"\n",
    "    idx = -1\n",
    "    for sent_idx, sent_span in enumerate(sentence_spans):\n",
    "            if sent_span[0] <= answer_span[0] and answer_span[1] <=sent_span[1]+1:\n",
    "                idx = sent_idx\n",
    "                break\n",
    "    return idx\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "work_dir = \"..\\\\data\\\\\"\n",
    "dev_filenames = [\"dev-v1.1.json\", \"train-v1.1.json\"]\n",
    "\n",
    "for dev_filename in dev_filenames:\n",
    "    with open(os.path.join(work_dir, dev_filename)) as data_file:    \n",
    "        dev_data = json.load(data_file)\n",
    "\n",
    "\n",
    "    print(\"train datafile info:\")\n",
    "    print(\"\\tversion:\", dev_data[\"version\"])\n",
    "    print()\n",
    "\n",
    "    data = dev_data[\"data\"]\n",
    "\n",
    "    sentences_context = []\n",
    "    sentences_question = []\n",
    "    for article in data:\n",
    "        title = article[\"title\"]\n",
    "        # print(title)\n",
    "        paragraphs = article[\"paragraphs\"]\n",
    "        for paragraph in paragraphs:\n",
    "            context = paragraph[\"context\"]\n",
    "            spans = get_sentence_spans(context)\n",
    "\n",
    "            # print context sentence by sentence\n",
    "            for idx, sent_span in enumerate(spans):\n",
    "                sentences_context.append(context[sent_span[0]:sent_span[1]+1])\n",
    "\n",
    "\n",
    "            qas = paragraph[\"qas\"]\n",
    "            for qa in qas:\n",
    "                id = qa[\"id\"]\n",
    "                question = qa[\"question\"]\n",
    "                answers = qa[\"answers\"]\n",
    "\n",
    "                sentences_question.append(question)\n",
    "\n",
    "print(\"total number of sentences extracted from context:\", len(sentences_context))\n",
    "print(\"total number of sentences extracted from question:\", len(sentences_question))\n",
    "sentences_question[:5]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert in into encoder decoder format\n",
    "\n",
    "Create a file that contains 3 tab separated entity.\n",
    "\n",
    "sentence1 sentence2 dummy_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"sentences_context.txt\", \"w\",encoding='utf-8') as f:\n",
    "    for sent in sentences_context:\n",
    "        f.write(sent+\"\\n\")\n",
    "        \n",
    "with open(\"sentences_question.txt\", \"w\",encoding='utf-8') as f:\n",
    "    for sent in sentences_question:\n",
    "        f.write(sent+\"\\n\")\n",
    "\n",
    "with open(\"sentences_all.txt\", \"w\",encoding='utf-8') as f:\n",
    "    for sent in sentences_context + sentences_question:\n",
    "        f.write(sent+\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
